{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = open(\"dev.csv\")         # dev set source-sink information\n",
    "dev_labels = open(\"dev-labels.csv\") # dev set  labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author[exampleid][venues]: {3, 19, 180, 20}\n",
      "author[exampleid][keywords]: {2, 35, 38, 6, 8, 42, 43, 47, 16, 18, 52, 24, 25, 30, 31}\n"
     ]
    }
   ],
   "source": [
    "# parse nodes.json\n",
    "author = defaultdict(dict) \n",
    "\n",
    "with open('nodes.json') as file:\n",
    "    data = json.load(file)\n",
    "    for instance in data:\n",
    "        author_id = int(instance[\"id\"])\n",
    "        author[author_id][\"first\"] = int(instance[\"first\"])\n",
    "        author[author_id][\"last\"] = int(instance[\"last\"])\n",
    "        author[author_id][\"num_paper\"] = int(instance[\"num_papers\"])\n",
    "        author[author_id][\"keywords\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'keyword' in key])\n",
    "        author[author_id][\"venues\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'venue' in key])\n",
    "        \n",
    "# access author data like this:\n",
    "example_id = 3\n",
    "print(\"author[exampleid][venues]:\", author[example_id][\"venues\"])\n",
    "print(\"author[exampleid][keywords]:\", author[example_id][\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse train.txt\n",
    "train = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "with open('train.txt') as file:\n",
    "    data = file.readlines()\n",
    "    for row in data:\n",
    "        tmp = row.split()\n",
    "        src = tmp[0]\n",
    "        sinks = tmp[1:]\n",
    "        for sink in sinks:\n",
    "            train[int(src)][int(sink)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a row of feautres given two author information\n",
    "colname = \"src_first,sink_first,src_last,sink_last,first_diff,last_diff,num_shared_keyword,num_shared_venue,edge\\n\"\n",
    "\n",
    "def constructRow(src, sink, label):\n",
    "    row = []\n",
    "    # years since first published\n",
    "    row.append(src['first'])\n",
    "    row.append(sink['first'])\n",
    "    # years since last published\n",
    "    row.append(src['last'])\n",
    "    row.append(sink['last'])\n",
    "    # difference in 'first' between two authors\n",
    "    row.append(src['first'] - sink['first'])\n",
    "    # difference in 'lasst' between two authors\n",
    "    row.append(src['last'] - sink['last'])\n",
    "\n",
    "    # number of overlapping keywords between two authors\n",
    "    common_keywords = src['keywords'].intersection(sink['keywords'])\n",
    "    row.append(len(common_keywords))\n",
    "    # number of overlapping venue between two authors\n",
    "    common_venues = src['venues'].intersection(sink['venues'])\n",
    "    row.append(len(common_venues))\n",
    "            \n",
    "    ## dont forget the labelling!\n",
    "    row.append(label)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct table (full)\n",
    "# table = []\n",
    "# # used to avoid repetition of sorce sink pairs\n",
    "# processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# # this nested loop constructs each row of the table\n",
    "# for src_id in author.keys():\n",
    "#     for sink_id in author.keys():\n",
    "#         # dont include edge to self, dont repeat edge\n",
    "#         if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "#             # generate row\n",
    "#             label = train[src_id][sink_id]\n",
    "#             row = constructRow(author[src_id], author[sink_id], label)\n",
    "#             # add row to table\n",
    "#             table.append(row)\n",
    "#             # set edge as processed\n",
    "#             processed[src_id][sink_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct table (stratified sampling, undersampling)\n",
    "table = []\n",
    "# used to avoid repetition of sorce sink pairs\n",
    "processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# first add only instance with links\n",
    "for src_id in train.keys():\n",
    "    for sink_id in train[src_id].keys():\n",
    "        # dont include edge to self, dont repeat edge\n",
    "        if (train[src_id][sink_id]) and (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "            # generate row\n",
    "            label = train[src_id][sink_id]\n",
    "            row = constructRow(author[src_id], author[sink_id], label)\n",
    "            # add row to table\n",
    "            table.append(row)\n",
    "            # set edge as processed\n",
    "            processed[src_id][sink_id] = 1\n",
    "\n",
    "# now add instances with no link\n",
    "to_add = len(table)\n",
    "while to_add > 0:\n",
    "    # pick random src and sink\n",
    "    src_id = random.randint(0, len(author.keys())-1)\n",
    "    sink_id = random.randint(0, len(author.keys())-1)\n",
    "    # dont include edge to self, dont repeat edge\n",
    "    if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "        # generate row\n",
    "        label = train[src_id][sink_id]\n",
    "        row = constructRow(author[src_id], author[sink_id], label)\n",
    "        # add row to table\n",
    "        table.append(row)\n",
    "        # set edge as processed\n",
    "        processed[src_id][sink_id] = 1\n",
    "        \n",
    "    to_add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('train_reconstructed.csv', 'w') as file:\n",
    "    ## header\n",
    "    file.write(colname)\n",
    "    for row in table:\n",
    "        row_string = \"\"\n",
    "        for i in range(len(row)):\n",
    "            features = row[i]\n",
    "            row_string += str(features)\n",
    "            if i+1 < len(row):\n",
    "                row_string += \",\"\n",
    "            else:\n",
    "                row_string += \"\\n\"\n",
    "                \n",
    "        file.write(row_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
