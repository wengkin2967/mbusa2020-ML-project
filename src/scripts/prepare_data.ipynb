{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author[exampleid][venues]: {3, 19, 180, 20}\n",
      "author[exampleid][keywords]: {2, 35, 38, 6, 8, 42, 43, 47, 16, 18, 52, 24, 25, 30, 31}\n"
     ]
    }
   ],
   "source": [
    "# parse nodes.json\n",
    "author = defaultdict(dict) \n",
    "\n",
    "with open('../data/raw/nodes.json') as file:\n",
    "    data = json.load(file)\n",
    "    for instance in data:\n",
    "        author_id = int(instance[\"id\"])\n",
    "        author[author_id][\"id\"] = author_id\n",
    "        author[author_id][\"first\"] = int(instance[\"first\"])\n",
    "        author[author_id][\"last\"] = int(instance[\"last\"])\n",
    "        author[author_id][\"num_papers\"] = int(instance[\"num_papers\"])\n",
    "        author[author_id][\"keywords\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'keyword' in key])\n",
    "        author[author_id][\"venues\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'venue' in key])\n",
    "        \n",
    "# access author data like this:\n",
    "example_id = 3\n",
    "print(\"author[exampleid][venues]:\", author[example_id][\"venues\"])\n",
    "print(\"author[exampleid][keywords]:\", author[example_id][\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse train.txt\n",
    "train = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "with open('../data/raw/train.txt') as file:\n",
    "    data = file.readlines()\n",
    "    for row in data:\n",
    "        tmp = row.split()\n",
    "        src = tmp[0]\n",
    "        sinks = tmp[1:]\n",
    "        for sink in sinks:\n",
    "            train[int(src)][int(sink)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dev\n",
    "dev = pd.read_csv(\"../data/raw/dev.csv\")         # dev set source-sink information\n",
    "dev_labels = pd.read_csv(\"../data/raw/dev-labels.csv\")['Expected'] # dev set  labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionary of neighbours of nodes (for dev)\n",
    "dev_src = dev['Source']\n",
    "dev_sink = dev['Sink']\n",
    "\n",
    "# Empty dictionary to store all the neighbours\n",
    "neighbours = {}\n",
    "\n",
    "# Stores each src's neighbours (Aggregate sinks) as a list in the neighbours dictionary\n",
    "for src in dev_src.unique():\n",
    "    neighbours[src] = set(dev[dev['Source'] == src]['Sink'])\n",
    "\n",
    "# Does the same for sink's neighbours (aggregate sources) as a list in the neighbours dictionary\n",
    "# adds on to the dictionary if key already exists.\n",
    "for sink in dev_sink.unique():\n",
    "    neighbours[sink] = neighbours.get(sink,set([])).union(dev[dev['Sink'] == sink]['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute minimum distance of two nodes (degrees of seperation). Uses BFS (UNUSED)\n",
    "# def compute_distance(graph, src_id, sink_id):\n",
    "    \n",
    "#     # record distance of visited path (used to check for visited nodes too)\n",
    "#     distance = {}\n",
    "#     # the queue\n",
    "#     distance[src_id] = 0\n",
    "#     queue = [src_id]\n",
    "\n",
    "#     # while queue is not empty\n",
    "#     while queue:\n",
    "#         # get a node from the queue\n",
    "#         current = queue.pop(0)\n",
    "#         # target found, return distance\n",
    "#         if current == sink_id:\n",
    "#             return distance[current]\n",
    "        \n",
    "#         # find neighbours and add them into the queue\n",
    "#         neighbours = graph[current]\n",
    "#         if neighbours:\n",
    "#             for n in neighbours:                \n",
    "#                 if n not in distance.keys():    \n",
    "#                     distance[n] = distance[current] + 1 # neighbour is always 1 step more\n",
    "#                     queue.append(n)\n",
    "\n",
    "#     return len(graph.keys())\n",
    "\n",
    "# testing\n",
    "# test_graph = {1:[2, 4],\n",
    "#              2:[1, 4, 3],\n",
    "#              3:[2, 6],\n",
    "#              4:[1, 2, 5],\n",
    "#              5:[4],\n",
    "#              6:[3],\n",
    "#              7:[]}\n",
    "\n",
    "# compute_distance(test_graph, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionary of neighbours of nodes (for train)\n",
    "# Dont need to consider for sink since train data neighbours is two way.\n",
    "train_neighbours = {}\n",
    "for src in train.keys():\n",
    "    train_neighbours[src] = set(train[src].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a row of feautres given two author information\n",
    "colname = [\"sum_papers\",\"first_diff\",\"last_diff\",\"overlap_years\",\"common_keywords\",\n",
    "           \"keyword_similarity\", \"common_venue\",\"venue_similarity\",\"common_neighbours\",\"neighbours_similarity\",\"edge\"]\n",
    "colname_str = \"\"\n",
    "for i in range(len(colname)):\n",
    "    colname_str += colname[i]\n",
    "    if i+1 < len(colname):\n",
    "        colname_str += \",\"\n",
    "    else:\n",
    "        colname_str += \"\\n\"\n",
    "\n",
    "def constructRow(src, sink, label=None):\n",
    "    row = []\n",
    "    # sum of papers\n",
    "    row.append(src['num_papers']+sink['num_papers'])\n",
    "    \n",
    "    # difference in 'first' between two authors\n",
    "    row.append(abs(src['first'] - sink['first']))\n",
    "    # difference in 'last' between two authors\n",
    "    row.append(abs(src['last'] - sink['last']))\n",
    "    \n",
    "    # overlap years\n",
    "    src_range = set(range(src['last'], src['first']))\n",
    "    sink_range = set(range(sink['last'], sink['first']))\n",
    "    row.append(len(src_range.intersection(sink_range)))\n",
    "    \n",
    "    # common words: number of overlapping keywords between two authors\n",
    "    common_keywords = src['keywords'].intersection(sink['keywords'])\n",
    "    row.append(len(common_keywords))\n",
    "    \n",
    "    # keyword similarity: overlap keywords / union numer of keywords\n",
    "    union_keywords = src['keywords'].union(sink['keywords'])\n",
    "    if len(union_keywords) != 0:\n",
    "        row.append(len(common_keywords) / len(union_keywords))\n",
    "    else: \n",
    "        row.append(0)\n",
    "    \n",
    "    # common venue: number of overlapping venue between two authors\n",
    "    common_venues = src['venues'].intersection(sink['venues'])\n",
    "    row.append(len(common_venues))\n",
    "    \n",
    "    # venue similarity: overlap venue / union numer of venue\n",
    "    union_venues = src['venues'].union(sink['venues'])\n",
    "    if len(union_venues) != 0:\n",
    "        row.append(len(common_venues) / len(union_venues))\n",
    "    else:\n",
    "        row.append(0)\n",
    "    \n",
    "    # common neighbours (accounts for nodes that are not recorded in training set)\n",
    "    common_neighbours = train_neighbours.get(src['id'],set([])).intersection(train_neighbours.get(sink['id'],set([])))\n",
    "    row.append(len(common_neighbours))\n",
    "    \n",
    "    # neighbour similarity: overlap neighbours / union number of neighbours\n",
    "    union_neighbours = train_neighbours.get(src['id'],set([])).union(train_neighbours.get(sink['id'],set([])))\n",
    "    row.append(len(common_neighbours)/len(union_neighbours)) if len(union_neighbours) else row.append(0)\n",
    "\n",
    "    ## dont forget the labelling!\n",
    "    if label:\n",
    "        row.append(label)\n",
    "    else:\n",
    "        row.append(0)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct table (full)\n",
    "# table = []\n",
    "# # used to avoid repetition of sorce sink pairs\n",
    "# processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# # this nested loop constructs each row of the table\n",
    "# for src_id in author.keys():\n",
    "#     for sink_id in author.keys():\n",
    "#         # dont include edge to self, dont repeat edge\n",
    "#         if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "#             # generate row\n",
    "#             label = train[src_id][sink_id]\n",
    "#             row = constructRow(author[src_id], author[sink_id], label)\n",
    "#             # add row to table\n",
    "#             table.append(row)\n",
    "#             # set edge as processed\n",
    "#             processed[src_id][sink_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct table (with undersampling)\n",
    "table = []\n",
    "# used to avoid repetition of sorce sink pairs\n",
    "processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# first add only instance with links\n",
    "for src_id in train.keys():\n",
    "    for sink_id in train[src_id].keys():\n",
    "        # dont include edge to self, dont repeat edge\n",
    "        if (train[src_id][sink_id]) and (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "            # generate row\n",
    "            label = train[src_id][sink_id]\n",
    "            row = constructRow(author[src_id], author[sink_id], label)\n",
    "            # add row to table\n",
    "            table.append(row)\n",
    "            # set edge as processed\n",
    "            processed[src_id][sink_id] = 1\n",
    "\n",
    "# now add instances with no link\n",
    "to_add = len(table)\n",
    "while to_add > 0:\n",
    "    # pick random src and sink\n",
    "    src_id = random.randint(0, len(author.keys())-1)\n",
    "    sink_id = random.randint(0, len(author.keys())-1)\n",
    "    # dont include edge to self, dont repeat edge\n",
    "    if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "        # generate row\n",
    "        label = train[src_id][sink_id]\n",
    "        row = constructRow(author[src_id], author[sink_id], label)\n",
    "        # add row to table\n",
    "        table.append(row)\n",
    "        # set edge as processed\n",
    "        processed[src_id][sink_id] = 1\n",
    "        \n",
    "    to_add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file (train)\n",
    "with open('../data/final/train_reconstructed.csv', 'w') as file:\n",
    "    ## header\n",
    "    file.write(colname_str)\n",
    "    for row in table:\n",
    "        row_string = \"\"\n",
    "        for i in range(len(row)):\n",
    "            features = row[i]\n",
    "            row_string += str(features)\n",
    "            if i+1 < len(row):\n",
    "                row_string += \",\"\n",
    "            else:\n",
    "                row_string += \"\\n\"\n",
    "                \n",
    "        file.write(row_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dev set and write to csv\n",
    "dev['Expected'] = dev_labels\n",
    "dev_src = dev['Source']\n",
    "dev_sink = dev['Sink']\n",
    "\n",
    "# total npapers\n",
    "sum_papers = np.array([author[dev_src[i]]['num_papers']  + \n",
    "                       author[dev_sink[i]]['num_papers'] for i in range(len(dev_src))])\n",
    "\n",
    "\n",
    "# # first year published\n",
    "src_first = np.array([author[id]['first'] for id in dev_src])\n",
    "sink_first = np.array([author[id]['first'] for id in dev_sink])\n",
    "\n",
    "# # last year published\n",
    "src_last = np.array([author[id]['last'] for id in dev_src])\n",
    "sink_last = np.array([author[id]['last'] for id in dev_sink])\n",
    "\n",
    "# Diff in first year and last year publishing between two authors.\n",
    "first_diff = abs(src_first - sink_first)\n",
    "last_diff = abs(src_last - sink_last)\n",
    "\n",
    "# overlap years\n",
    "src_range = [set(range(author[src]['last'], author[src]['first']))\n",
    "              for src in dev_src]\n",
    "sink_range = [set(range(author[sink]['last'], author[sink]['first']))\n",
    "              for sink in dev_sink]\n",
    "overlap_years = [len(src_range[i].intersection(sink_range[i])) for i in range(len(src_range))]\n",
    "\n",
    "# overlap keywords / union numer of keywords\n",
    "common_keywords = np.array([len(author[dev_src[i]]['keywords'].intersection(author[dev_sink[i]]['keywords'])) for i in range(len(dev_sink))])\n",
    "union_keywords = np.array([len(author[dev_src[i]]['keywords'].union(author[dev_sink[i]]['keywords'])) for i in range(len(dev_sink))])\n",
    "keyword_similarity = common_keywords / union_keywords\n",
    "\n",
    "# overlapping venus/ union number of venues\n",
    "common_venue = np.array([len(author[dev_src[i]]['venues'].intersection(author[dev_sink[i]]['venues'])) for i in range(len(dev_sink))])\n",
    "union_venues = np.array([len(author[dev_src[i]]['venues'].union(author[dev_sink[i]]['venues'])) for i in range(len(dev_sink))])\n",
    "venue_similarity = common_venue / union_venues \n",
    "\n",
    "# common neighbours\n",
    "common_neighbours = np.array([len(neighbours[dev_src[i]].intersection(neighbours[dev_sink[i]])) for i in range(len(dev_sink))])\n",
    "union_neighbours =  np.array([len(neighbours[dev_src[i]].union(neighbours[dev_sink[i]])) for i in range(len(dev_sink))])\n",
    "neighbours_similarity = common_neighbours/union_neighbours\n",
    "\n",
    "edge = dev['Expected'].apply(lambda x: 0 if x == -1 else x)\n",
    "\n",
    "test_df = { 'sum_papers':sum_papers,\n",
    "            'first_diff': first_diff,\n",
    "            'last_diff': last_diff,\n",
    "            'overlap_years': overlap_years,\n",
    "            'common_keywords': common_keywords,\n",
    "            'keyword_similarity': keyword_similarity,\n",
    "            'common_venue': common_venue,\n",
    "            'venue_similarity': venue_similarity,\n",
    "            'common_neighbours': common_neighbours,\n",
    "            'neighbours_similarity': neighbours_similarity,\n",
    "            'edge': edge \n",
    "            }\n",
    "dev_test = pd.DataFrame(data=test_df)\n",
    "dev_test.to_csv('../data/final/dev-test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing test-public.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse test-public\n",
    "test_final = pd.read_csv(\"../data/raw/test-public.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionary of neighbours of nodes (for test_final)\n",
    "test_final_src = test_final['Source']\n",
    "test_final_sink = test_final['Sink']\n",
    "\n",
    "# Empty dictionary to store all the neighbours\n",
    "neighbours = {}\n",
    "\n",
    "# Stores each src's neighbours (Aggregate sinks) as a list in the neighbours dictionary\n",
    "for src in test_final_src.unique():\n",
    "    neighbours[src] = set(test_final[test_final['Source'] == src]['Sink'])\n",
    "\n",
    "# Does the same for sink's neighbours (aggregate sources) as a list in the neighbours dictionary\n",
    "# adds on to the dictionary if key already exists.\n",
    "for sink in test_final_sink.unique():\n",
    "    neighbours[sink] = neighbours.get(sink,set([])).union(test_final[test_final['Sink'] == sink]['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-571a7123173b>:37: RuntimeWarning: invalid value encountered in true_divide\n",
      "  venue_similarity = common_venue / union_venues\n"
     ]
    }
   ],
   "source": [
    "# create final-test set and write to csv\n",
    "test_final_src = test_final['Source']\n",
    "test_final_sink = test_final['Sink']\n",
    "\n",
    "# total npapers\n",
    "sum_papers = np.array([author[test_final_src[i]]['num_papers']  + \n",
    "                       author[test_final_sink[i]]['num_papers'] for i in range(len(test_final_src))])\n",
    "\n",
    "\n",
    "# # first year published\n",
    "src_first = np.array([author[id]['first'] for id in test_final_src])\n",
    "sink_first = np.array([author[id]['first'] for id in test_final_sink])\n",
    "\n",
    "# # last year published\n",
    "src_last = np.array([author[id]['last'] for id in test_final_src])\n",
    "sink_last = np.array([author[id]['last'] for id in test_final_sink])\n",
    "\n",
    "# Diff in first year and last year publishing between two authors.\n",
    "first_diff = abs(src_first - sink_first)\n",
    "last_diff = abs(src_last - sink_last)\n",
    "\n",
    "# overlap years\n",
    "src_range = [set(range(author[src]['last'], author[src]['first']))\n",
    "              for src in test_final_src]\n",
    "sink_range = [set(range(author[sink]['last'], author[sink]['first']))\n",
    "              for sink in test_final_sink]\n",
    "overlap_years = [len(src_range[i].intersection(sink_range[i])) for i in range(len(src_range))]\n",
    "\n",
    "# overlap keywords / union numer of keywords\n",
    "common_keywords = np.array([len(author[test_final_src[i]]['keywords'].intersection(author[test_final_sink[i]]['keywords'])) for i in range(len(test_final_sink))])\n",
    "union_keywords = np.array([len(author[test_final_src[i]]['keywords'].union(author[test_final_sink[i]]['keywords'])) for i in range(len(test_final_sink))])\n",
    "keyword_similarity = common_keywords / union_keywords\n",
    "\n",
    "# overlapping venus/ union number of venues\n",
    "common_venue = np.array([len(author[test_final_src[i]]['venues'].intersection(author[test_final_sink[i]]['venues'])) for i in range(len(test_final_sink))])\n",
    "union_venues = np.array([len(author[test_final_src[i]]['venues'].union(author[test_final_sink[i]]['venues'])) for i in range(len(test_final_sink))])\n",
    "venue_similarity = common_venue / union_venues\n",
    "\n",
    "# common neighbours\n",
    "common_neighbours = np.array([len(neighbours[test_final_src[i]].intersection(neighbours[test_final_sink[i]])) for i in range(len(test_final_sink))])\n",
    "union_neighbours =  np.array([len(neighbours[test_final_src[i]].union(neighbours[test_final_sink[i]])) for i in range(len(test_final_sink))])\n",
    "neighbours_similarity = common_neighbours/union_neighbours\n",
    "\n",
    "test_df = { 'sum_papers':sum_papers,\n",
    "            'first_diff': first_diff,\n",
    "            'last_diff': last_diff,\n",
    "            'overlap_years': overlap_years,\n",
    "            'common_keywords': common_keywords,\n",
    "            'keyword_similarity': keyword_similarity,\n",
    "            'common_venue': common_venue,\n",
    "            'venue_similarity': venue_similarity,\n",
    "            'common_neighbours': common_neighbours,\n",
    "            'neighbours_similarity': neighbours_similarity,\n",
    "            }\n",
    "test_final_df = pd.DataFrame(data=test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with one null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_papers</th>\n",
       "      <th>first_diff</th>\n",
       "      <th>last_diff</th>\n",
       "      <th>overlap_years</th>\n",
       "      <th>common_keywords</th>\n",
       "      <th>keyword_similarity</th>\n",
       "      <th>common_venue</th>\n",
       "      <th>venue_similarity</th>\n",
       "      <th>common_neighbours</th>\n",
       "      <th>neighbours_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sum_papers  first_diff  last_diff  overlap_years  common_keywords  \\\n",
       "1035           2           0          0              0                1   \n",
       "\n",
       "      keyword_similarity  common_venue  venue_similarity  common_neighbours  \\\n",
       "1035            0.111111             0               NaN                  0   \n",
       "\n",
       "      neighbours_similarity  \n",
       "1035                    0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_df[test_final_df['venue_similarity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_papers</th>\n",
       "      <th>first_diff</th>\n",
       "      <th>last_diff</th>\n",
       "      <th>overlap_years</th>\n",
       "      <th>common_keywords</th>\n",
       "      <th>keyword_similarity</th>\n",
       "      <th>common_venue</th>\n",
       "      <th>venue_similarity</th>\n",
       "      <th>common_neighbours</th>\n",
       "      <th>neighbours_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sum_papers, first_diff, last_diff, overlap_years, common_keywords, keyword_similarity, common_venue, venue_similarity, common_neighbours, neighbours_similarity]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_df['venue_similarity'].fillna(0,inplace=True)\n",
    "test_final_df[test_final_df['venue_similarity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_final_df.to_csv('../data/final/test-final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
