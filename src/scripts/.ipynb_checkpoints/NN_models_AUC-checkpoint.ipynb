{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "np.random.seed(90501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# train set\n",
    "train_set = pd.read_csv('../data/final/train_reconstructed.csv')\n",
    "X_train = train_set.iloc[:,:-1].values\n",
    "y_train = train_set['edge'].values\n",
    "\n",
    "# dev set\n",
    "test_set = pd.read_csv('../data/final/dev-test.csv')\n",
    "X_test = test_set.iloc[:,:-1].values\n",
    "y_test = test_set['edge'].values\n",
    "\n",
    "X_train = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "y_train = to_categorical(y_train, 2)\n",
    "y_test = to_categorical(y_test, 2)\n",
    "\n",
    "X_flatten_train = [instance.flatten() for instance in X_train]\n",
    "X_flatten_test = [instance.flatten() for instance in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using one layer and seeing what happens gives us a good idea of what the simplest model could give us in terms of accuracy (around 72% - 78%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\users\\janze\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "744/744 [==============================] - 1s 834us/step - loss: 0.1984 - auc: 0.9774\n",
      "Epoch 2/20\n",
      "744/744 [==============================] - 1s 830us/step - loss: 0.1695 - auc: 0.9821\n",
      "Epoch 3/20\n",
      "744/744 [==============================] - 1s 814us/step - loss: 0.1692 - auc: 0.9822\n",
      "Epoch 4/20\n",
      "744/744 [==============================] - 1s 810us/step - loss: 0.1665 - auc: 0.9826\n",
      "Epoch 5/20\n",
      "744/744 [==============================] - 1s 823us/step - loss: 0.1690 - auc: 0.9823\n",
      "Epoch 6/20\n",
      "744/744 [==============================] - 1s 811us/step - loss: 0.1663 - auc: 0.9830\n",
      "Epoch 7/20\n",
      "744/744 [==============================] - 1s 818us/step - loss: 0.1665 - auc: 0.9829\n",
      "Epoch 8/20\n",
      "744/744 [==============================] - 1s 818us/step - loss: 0.1636 - auc: 0.9833\n",
      "Epoch 9/20\n",
      "744/744 [==============================] - 1s 863us/step - loss: 0.1615 - auc: 0.9838\n",
      "Epoch 10/20\n",
      "744/744 [==============================] - 1s 853us/step - loss: 0.1644 - auc: 0.9831\n",
      "Epoch 11/20\n",
      "744/744 [==============================] - 1s 808us/step - loss: 0.1628 - auc: 0.9835\n",
      "Epoch 12/20\n",
      "744/744 [==============================] - 1s 830us/step - loss: 0.1600 - auc: 0.9839\n",
      "Epoch 13/20\n",
      "744/744 [==============================] - 1s 815us/step - loss: 0.1579 - auc: 0.9843\n",
      "Epoch 14/20\n",
      "744/744 [==============================] - 1s 838us/step - loss: 0.1602 - auc: 0.9839\n",
      "Epoch 15/20\n",
      "744/744 [==============================] - 1s 824us/step - loss: 0.1607 - auc: 0.9838\n",
      "Epoch 16/20\n",
      "744/744 [==============================] - 1s 807us/step - loss: 0.1633 - auc: 0.9835\n",
      "Epoch 17/20\n",
      "744/744 [==============================] - 1s 815us/step - loss: 0.1624 - auc: 0.9836\n",
      "Epoch 18/20\n",
      "744/744 [==============================] - 1s 812us/step - loss: 0.1585 - auc: 0.9842\n",
      "Epoch 19/20\n",
      "744/744 [==============================] - 1s 820us/step - loss: 0.1601 - auc: 0.9838\n",
      "Epoch 20/20\n",
      "744/744 [==============================] - 1s 812us/step - loss: 0.1572 - auc: 0.9844\n",
      "39/39 [==============================] - 0s 818us/step - loss: 1.1943 - auc: 0.7699\n",
      "\n",
      "On the test set: the loss is 1.1942864656448364 and the AUC is 0.7699442505836487\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "nnmodel_1 = Sequential()\n",
    "nnmodel_1.add(Flatten(input_shape= X_train.shape[1:]))\n",
    "nnmodel_1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "nnmodel_1.compile(loss='categorical_crossentropy', optimizer = SGD(lr=0.01, decay =2e-6, momentum =0.9, nesterov = True),\n",
    "                 metrics = ['AUC'])\n",
    "\n",
    "nnmodel_1.summary()\n",
    "nnmodel_1.fit(X_train, y_train, epochs = 20, batch_size = 128)\n",
    "\n",
    "score = nnmodel_1.evaluate(X_test, y_test, batch_size = 128)\n",
    "\n",
    "print(\"\\nOn the test set: the loss is {} and the AUC is {}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set a more complex model testing these activation functions: **['relu', 'tanh', 'sigmoid', 'selu', 'elu']**. Using two layers of 11 neurons each. We get that usually the best activation function is **tanh** with nearly **78% - 80%** of the accuracy in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\USUARIO\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.2963 - auc: 0.9569\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1458 - auc: 0.9846\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1393 - auc: 0.9857\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1356 - auc: 0.9865\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1334 - auc: 0.9871\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1307 - auc: 0.9876\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1286 - auc: 0.9880\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1279 - auc: 0.9883\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1271 - auc: 0.9883\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1260 - auc: 0.9886\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1248 - auc: 0.9889\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1246 - auc: 0.9889\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1234 - auc: 0.9891\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1228 - auc: 0.9892\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1222 - auc: 0.9893\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1218 - auc: 0.9893\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1215 - auc: 0.9894\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1213 - auc: 0.9894\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1212 - auc: 0.9894\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1210 - auc: 0.9894\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7197 - auc: 0.8168\n",
      "[0.7196690440177917, 0.8168138265609741]\n",
      "On the test set: the loss is 0.7196690440177917 and the accuracy is 0.8168138265609741\n",
      "Max accuracy so far 0.8168138265609741 in iteration relu\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 24        \n",
      "=================================================================\n",
      "Total params: 332\n",
      "Trainable params: 332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.2125 - auc: 0.9725\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1348 - auc: 0.9868\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1301 - auc: 0.9878\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1278 - auc: 0.9882\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1262 - auc: 0.9885\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1252 - auc: 0.9887\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1235 - auc: 0.9890\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1222 - auc: 0.9894\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1218 - auc: 0.9894\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1220 - auc: 0.9894\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1214 - auc: 0.9893\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1211 - auc: 0.9894\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1208 - auc: 0.9896\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1215 - auc: 0.9895\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1199 - auc: 0.9896\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1201 - auc: 0.9896\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1195 - auc: 0.9897\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1193 - auc: 0.9897\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1195 - auc: 0.9897\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1190 - auc: 0.9898\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7276 - auc: 0.8277\n",
      "[0.7275850772857666, 0.8276915550231934]\n",
      "On the test set: the loss is 0.7275850772857666 and the accuracy is 0.8276915550231934\n",
      "Max accuracy so far 0.8276915550231934 in iteration tanh\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 24        \n",
      "=================================================================\n",
      "Total params: 332\n",
      "Trainable params: 332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.4067 - auc: 0.9274\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1475 - auc: 0.9858\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1298 - auc: 0.9878\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1248 - auc: 0.9887\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1230 - auc: 0.9890\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1217 - auc: 0.9893\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1208 - auc: 0.9895\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1213 - auc: 0.9894\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1198 - auc: 0.9897\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1199 - auc: 0.9897\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1195 - auc: 0.9898\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1191 - auc: 0.9899\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1190 - auc: 0.9899\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1187 - auc: 0.9899\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1182 - auc: 0.9900\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1191 - auc: 0.9900\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1183 - auc: 0.9900\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1178 - auc: 0.9902\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1177 - auc: 0.9902\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1173 - auc: 0.9902\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7329 - auc: 0.8225\n",
      "[0.7329397797584534, 0.8225263357162476]\n",
      "On the test set: the loss is 0.7329397797584534 and the accuracy is 0.8225263357162476\n",
      "Max accuracy so far 0.8276915550231934 in iteration tanh\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 24        \n",
      "=================================================================\n",
      "Total params: 332\n",
      "Trainable params: 332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1944 - auc: 0.9779\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1283 - auc: 0.9885\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1250 - auc: 0.9889\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1230 - auc: 0.9892\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1217 - auc: 0.9895\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1210 - auc: 0.9895\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1205 - auc: 0.9897\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1199 - auc: 0.9898\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1194 - auc: 0.9898\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1183 - auc: 0.9900\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1180 - auc: 0.9901\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1179 - auc: 0.9901\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1172 - auc: 0.9902\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1175 - auc: 0.9901\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1172 - auc: 0.9902\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1172 - auc: 0.9902\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1165 - auc: 0.9904\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1167 - auc: 0.9903\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1161 - auc: 0.9903\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1160 - auc: 0.9904\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7749 - auc: 0.8211\n",
      "[0.7748661637306213, 0.8210783004760742]\n",
      "On the test set: the loss is 0.7748661637306213 and the accuracy is 0.8210783004760742\n",
      "Max accuracy so far 0.8276915550231934 in iteration tanh\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 24        \n",
      "=================================================================\n",
      "Total params: 332\n",
      "Trainable params: 332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.2327 - auc: 0.9762\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1266 - auc: 0.9884\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1226 - auc: 0.9892\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1213 - auc: 0.9895\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1208 - auc: 0.9896\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1194 - auc: 0.9899\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1183 - auc: 0.9901\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1180 - auc: 0.9902\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1182 - auc: 0.9901\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1174 - auc: 0.9903\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1173 - auc: 0.9902\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1169 - auc: 0.9904\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1169 - auc: 0.9904\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1165 - auc: 0.9904\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1162 - auc: 0.9904\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1162 - auc: 0.9904\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1158 - auc: 0.9904\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 3ms/step - loss: 0.1158 - auc: 0.9905\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1154 - auc: 0.9906\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 1s 2ms/step - loss: 0.1155 - auc: 0.9905\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6426 - auc: 0.8350\n",
      "[0.6425802111625671, 0.835031270980835]\n",
      "On the test set: the loss is 0.6425802111625671 and the accuracy is 0.835031270980835\n",
      "Max accuracy so far 0.835031270980835 in iteration elu\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 11)                176       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 11)                132       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 24        \n",
      "=================================================================\n",
      "Total params: 332\n",
      "Trainable params: 332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "max_accuracy = 0\n",
    "for item in ['relu', 'tanh', 'sigmoid', 'selu', 'elu']:\n",
    "    nnmodel_2 = Sequential()\n",
    "    nnmodel_2.add(Flatten(input_shape = X_train.shape[1:]))\n",
    "    nnmodel_2.add(Dense(11, activation = item))\n",
    "    nnmodel_2.add(Dense(11, activation = item))\n",
    "    nnmodel_2.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    nnmodel_2.compile(loss='categorical_crossentropy',\n",
    "             optimizer = SGD(lr = 0.01, decay = 1e-6, momentum=0.9, nesterov = True),\n",
    "             metrics = ['AUC'])\n",
    "\n",
    "    nnmodel_2.fit(X_train, y_train, epochs = 20, batch_size = 256)\n",
    "\n",
    "    score = nnmodel_2.evaluate(X_test, y_test, batch_size = 256)\n",
    "    print(score)\n",
    "    print(\"On the test set: the loss is {} and the AUC is {}\".format(score[0], score[1]))\n",
    "    accuracy = score[1]\n",
    "    if accuracy >= max_accuracy:\n",
    "        max_accuracy = accuracy\n",
    "        iteration_max = item\n",
    "    print(f\"Max accuracy so far {max_accuracy} in iteration {iteration_max}\")\n",
    "    nnmodel_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we decide to tune the hyperparameters using KJerasClassifier and GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function, the model is created according to the specifications that we give as an input. This is very handy because we now can use the function GridSearchCV in order to get the \"best\" hyperparameters out of the set that we provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers, activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(nodes, input_dim = X_train.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "             optimizer = 'adam',\n",
    "             metrics = ['AUC'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "for i in range(1, 4):\n",
    "    sublayer = []\n",
    "    sublayer.append([list(i) for i in itertools.combinations_with_replacement(range(8,14), i)])\n",
    "    for item in sublayer[0]:\n",
    "        layers.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "param_grid = dict(layers=layers, activation = activations, batch_size = [128], epochs = [20])\n",
    "grid = GridSearchCV(estimator = model, param_grid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous chunck of code, we get that the most accurare parameters are the following. Nevertheless, it is not better at fitting the training data than the one selected in previous steps (0.96 accuracy vs 0.95 this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9504482170173003,\n",
       " {'activation': 'tanh', 'batch_size': 128, 'epochs': 20, 'layers': [8, 8, 8]}]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[grid_result.best_score_, grid_result.best_params_]\n",
    "#[0.9504482170173003, {'activation': 'tanh', 'batch_size': 128, 'epochs': 20, 'layers': [8, 8, 8]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using values above, we now can get the \"best fit\" of those hyperparameters tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.2961 - auc: 0.9455\n",
      "Epoch 2/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.2064 - auc: 0.9664\n",
      "Epoch 3/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1909 - auc: 0.9685\n",
      "Epoch 4/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1856 - auc: 0.9697\n",
      "Epoch 5/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1846 - auc: 0.9700\n",
      "Epoch 6/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1771 - auc: 0.9713\n",
      "Epoch 7/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1791 - auc: 0.9710\n",
      "Epoch 8/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1743 - auc: 0.9717\n",
      "Epoch 9/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1747 - auc: 0.9716\n",
      "Epoch 10/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1751 - auc: 0.9713\n",
      "Epoch 11/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1722 - auc: 0.9720\n",
      "Epoch 12/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1727 - auc: 0.9724\n",
      "Epoch 13/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1720 - auc: 0.9717\n",
      "Epoch 14/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1703 - auc: 0.9725\n",
      "Epoch 15/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1720 - auc: 0.9722\n",
      "Epoch 16/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1698 - auc: 0.9732\n",
      "Epoch 17/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1698 - auc: 0.9733\n",
      "Epoch 18/20\n",
      "372/372 [==============================] - 1s 1ms/step - loss: 0.1691 - auc: 0.9732\n",
      "Epoch 19/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1690 - auc: 0.9736\n",
      "Epoch 20/20\n",
      "372/372 [==============================] - 0s 1ms/step - loss: 0.1676 - auc: 0.9731\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7103 - auc: 0.8569\n",
      "On the test set: the loss is 0.7103415727615356 and the AUC is 0.8569408655166626\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "nnmodel_3 = Sequential()\n",
    "nnmodel_3.add(Flatten(input_shape = X_train.shape[1:]))\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "nnmodel_3.compile(loss='binary_crossentropy',\n",
    "         optimizer = SGD(lr = 0.01, decay = 1e-6, momentum=0.9, nesterov = True),\n",
    "         metrics = ['AUC'])\n",
    "\n",
    "nnmodel_3.fit(X_train, y_train, epochs = 20, batch_size = 256)\n",
    "\n",
    "score = nnmodel_3.evaluate(X_test, y_test, batch_size = 256)\n",
    "\n",
    "print(\"On the test set: the loss is {} and the AUC is {}\".format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7103415727615356, 0.8569408655166626]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_test:0.8683\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_test_raw = test_set['edge'].values\n",
    "y_test_predict = nnmodel_3.predict(X_test)\n",
    "\n",
    "# get roc fpr and tpr\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_raw, y_test_predict[:,1])\n",
    "\n",
    "# results\n",
    "print(\"AUC_test:{:.4f}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After runing the test againg, as stated previously, this model does not now an improvement against nnmodel_2 and that is the best NN model explored at the moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# train set\n",
    "train_set = pd.read_csv('../data/final/train_reconstructed.csv')\n",
    "X_train = train_set.iloc[:,:-1].values\n",
    "y_train = train_set['edge'].values\n",
    "\n",
    "# dev set\n",
    "test_set = pd.read_csv('../data/final/dev-test.csv')\n",
    "X_test = test_set.iloc[:,:-1].values\n",
    "y_test = test_set['edge'].values\n",
    "\n",
    "X_big = np.concatenate((X_train,X_test),0)\n",
    "y_big = np.concatenate((y_train,y_test),0)\n",
    "\n",
    "\n",
    "#X_big = X_big.reshape(-1, X_big.shape[1], 1)\n",
    "y_big = to_categorical(y_big, 2)\n",
    "X_flatten_big = [instance.flatten() for instance in X_big]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training full model\n",
    "test_final = pd.read_csv('../data/final/test-final.csv')\n",
    "test_final = test_final.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.3173 - auc: 0.9381\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2133 - auc: 0.9646\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.2078 - auc: 0.9646\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1996 - auc: 0.9661\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1980 - auc: 0.9669\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1963 - auc: 0.9666\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1969 - auc: 0.9673\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1961 - auc: 0.9672\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1953 - auc: 0.9670\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1933 - auc: 0.9673\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1912 - auc: 0.9683\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1907 - auc: 0.9691\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1899 - auc: 0.9693\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1902 - auc: 0.9692\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1889 - auc: 0.9693\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1861 - auc: 0.9706\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1873 - auc: 0.9701\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1875 - auc: 0.9704\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1880 - auc: 0.9701\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 1s 1ms/step - loss: 0.1865 - auc: 0.9708\n",
      "391/391 [==============================] - 0s 1ms/step - loss: 0.1586 - auc: 0.9827\n",
      "On the test set: the loss is 0.15855763852596283 and the AUC is 0.9827247262001038\n"
     ]
    }
   ],
   "source": [
    "nnmodel_3 = Sequential()\n",
    "nnmodel_3.add(Flatten(input_shape = X_big.shape[1:])) # here\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(8, activation = \"tanh\"))\n",
    "nnmodel_3.add(Dropout(0.3))\n",
    "nnmodel_3.add(Dense(2, activation = \"softmax\"))\n",
    "\n",
    "nnmodel_3.compile(loss='binary_crossentropy',\n",
    "         optimizer = SGD(lr = 0.01, decay = 1e-6, momentum=0.9, nesterov = True),\n",
    "         metrics = ['AUC'])\n",
    "\n",
    "nnmodel_3.fit(X_big, y_big, epochs = 20, batch_size = 256)  # here\n",
    "\n",
    "score = nnmodel_3.evaluate(X_big, y_big, batch_size = 256)   # here\n",
    "\n",
    "print(\"On the test set: the loss is {} and the AUC is {}\".format(score[0], score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nnmodel_3.predict(test_final)[:,1]\n",
    "submission = {\n",
    "    'Id': range(1,len(pred)+1),\n",
    "    'Predicted': pred\n",
    "}\n",
    "\n",
    "submission_df = pd.DataFrame(data=submission)\n",
    "submission_df.to_csv('../data/final/sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
