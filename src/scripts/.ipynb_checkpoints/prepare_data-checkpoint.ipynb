{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"../data/raw/dev.csv\")         # dev set source-sink information\n",
    "dev_labels = pd.read_csv(\"../data/raw/dev-labels.csv\")['Expected'] # dev set  labelling\n",
    "\n",
    "dev['Expected'] = dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author[exampleid][venues]: {3, 19, 180, 20}\n",
      "author[exampleid][keywords]: {2, 35, 38, 6, 8, 42, 43, 47, 16, 18, 52, 24, 25, 30, 31}\n"
     ]
    }
   ],
   "source": [
    "# parse nodes.json\n",
    "author = defaultdict(dict) \n",
    "\n",
    "with open('../data/raw/nodes.json') as file:\n",
    "    data = json.load(file)\n",
    "    for instance in data:\n",
    "        author_id = int(instance[\"id\"])\n",
    "        author[author_id][\"id\"] = author_id\n",
    "        author[author_id][\"first\"] = int(instance[\"first\"])\n",
    "        author[author_id][\"last\"] = int(instance[\"last\"])\n",
    "        author[author_id][\"num_papers\"] = int(instance[\"num_papers\"])\n",
    "        author[author_id][\"keywords\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'keyword' in key])\n",
    "        author[author_id][\"venues\"] = set([int(key.split('_')[1]) for key in instance.keys() if 'venue' in key])\n",
    "        \n",
    "# access author data like this:\n",
    "example_id = 3\n",
    "print(\"author[exampleid][venues]:\", author[example_id][\"venues\"])\n",
    "print(\"author[exampleid][keywords]:\", author[example_id][\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionary of neighbours of nodes (for dev)\n",
    "dev_src = dev['Source']\n",
    "dev_sink = dev['Sink']\n",
    "\n",
    "# Empty dictionary to store all the neighbours\n",
    "neighbours = {}\n",
    "\n",
    "# Stores each src's neighbours (Aggregate sinks) as a list in the neighbours dictionary\n",
    "for src in dev_src.unique():\n",
    "    neighbours[src] = set(dev[dev['Source'] == src]['Sink'])\n",
    "\n",
    "# Does the same for sink's neighbours (aggregate sources) as a list in the neighbours dictionary\n",
    "# adds on to the dictionary if key already exists.\n",
    "for sink in dev_sink.unique():\n",
    "    neighbours[sink] = neighbours.get(sink,set([])).union(dev[dev['Sink'] == sink]['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute minimum distance of two nodes (degrees of seperation). Uses BFS\n",
    "def compute_distance(graph, src_id, sink_id):\n",
    "    \n",
    "    # record distance of visited path (used to check for visited nodes too)\n",
    "    distance = {}\n",
    "    # the queue\n",
    "    distance[src_id] = 0\n",
    "    queue = [src_id]\n",
    "\n",
    "    # while queue is not empty\n",
    "    while queue:\n",
    "        # get a node from the queue\n",
    "        current = queue.pop(0)\n",
    "        # target found, return distance\n",
    "        if current == sink_id:\n",
    "            return distance[current]\n",
    "        \n",
    "        # find neighbours and add them into the queue\n",
    "        neighbours = graph[current]\n",
    "        if neighbours:\n",
    "            for n in neighbours:                \n",
    "                if n not in distance.keys():    \n",
    "                    distance[n] = distance[current] + 1 # neighbour is always 1 step more\n",
    "                    queue.append(n)\n",
    "\n",
    "    return len(graph.keys())\n",
    "\n",
    "# testing\n",
    "# test_graph = {1:[2, 4],\n",
    "#              2:[1, 4, 3],\n",
    "#              3:[2, 6],\n",
    "#              4:[1, 2, 5],\n",
    "#              5:[4],\n",
    "#              6:[3],\n",
    "#              7:[]}\n",
    "\n",
    "# compute_distance(test_graph, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dev table\n",
    "dev_src = dev['Source']\n",
    "dev_sink = dev['Sink']\n",
    "\n",
    "# npapers\n",
    "npapers_src = np.array([author[id]['num_papers'] for id in dev_src])\n",
    "npapers_sink =  np.array([author[id]['num_papers'] for id in dev_sink])\n",
    "\n",
    "# first year published\n",
    "src_first = np.array([author[id]['first'] for id in dev_src])\n",
    "sink_first = np.array([author[id]['first'] for id in dev_sink])\n",
    "\n",
    "# last year published\n",
    "src_last = np.array([author[id]['last'] for id in dev_src])\n",
    "sink_last = np.array([author[id]['last'] for id in dev_sink])\n",
    "\n",
    "# Diff in first year and last year publishing between two authors.\n",
    "first_diff = src_first - sink_first\n",
    "last_diff = src_last - sink_last\n",
    "\n",
    "# overlap years\n",
    "src_range = [set(range(author[src]['first'], author[src]['last']))\n",
    "              for src in dev_src]\n",
    "sink_range = [set(range(author[sink]['first'], author[sink]['last']))\n",
    "              for sink in dev_sink]\n",
    "overlap_years = [len(src_range[i].intersection(sink_range[i])) for i in range(len(src_range))]\n",
    "\n",
    "\n",
    "\n",
    "# overlap keywords / union numer of keywords\n",
    "common_keywords = np.array([len(author[dev_src[i]]['keywords'].intersection(author[dev_sink[i]]['keywords'])) for i in range(len(dev_sink))])\n",
    "union_keywords = np.array([len(author[dev_src[i]]['keywords'].union(author[dev_sink[i]]['keywords'])) for i in range(len(dev_sink))])\n",
    "keyword_similarity = common_keywords / union_keywords\n",
    "\n",
    "# overlapping venus/ union number of venues\n",
    "common_venue = np.array([len(author[dev_src[i]]['venues'].intersection(author[dev_sink[i]]['venues'])) for i in range(len(dev_sink))])\n",
    "union_venues = np.array([len(author[dev_src[i]]['venues'].union(author[dev_sink[i]]['venues'])) for i in range(len(dev_sink))])\n",
    "venue_similarity = common_venue / union_venues\n",
    "\n",
    "# common neighbours\n",
    "common_neighbours = np.array([len(neighbours[dev_src[i]].intersection(neighbours[dev_sink[i]])) for i in range(len(dev_sink))])\n",
    "union_neighbours =  np.array([len(neighbours[dev_src[i]].union(neighbours[dev_sink[i]])) for i in range(len(dev_sink))])\n",
    "neighbours_similarity = common_neighbours/union_neighbours\n",
    "\n",
    "edge = dev['Expected'].apply(lambda x: 0 if x == -1 else x)\n",
    "\n",
    "test_df = {\n",
    "            'npapers_src': npapers_src,\n",
    "            'npapers_sink': npapers_sink,\n",
    "            'src_first': src_first,\n",
    "            'sink_first': sink_first,\n",
    "            'src_last': src_last,\n",
    "            'sink_last': sink_last,\n",
    "            'first_diff': first_diff,\n",
    "            'last_diff': last_diff,\n",
    "            'overlap_years': overlap_years,\n",
    "            'common_keywords': common_keywords,\n",
    "            'keyword_similarity': keyword_similarity,\n",
    "            'common_venue': common_venue,\n",
    "            'venue_similarity': venue_similarity,\n",
    "            'common_neighbours': common_neighbours,\n",
    "            'neighbours_similarity': neighbours_similarity,\n",
    "            'edge': edge \n",
    "            }\n",
    "dev_test = pd.DataFrame(data=test_df)\n",
    "dev_test.to_csv('../data/final/dev-test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse train.txt\n",
    "train = defaultdict(lambda: defaultdict(int)) \n",
    "\n",
    "with open('../data/raw/train.txt') as file:\n",
    "    data = file.readlines()\n",
    "    for row in data:\n",
    "        tmp = row.split()\n",
    "        src = tmp[0]\n",
    "        sinks = tmp[1:]\n",
    "        for sink in sinks:\n",
    "            train[int(src)][int(sink)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise dictionary of neighbours of nodes (for train)\n",
    "# Dont need to consider for sink since train data neighbours is two way.\n",
    "\n",
    "train_neighbours = {}\n",
    "for src in train.keys():\n",
    "    train_neighbours[src] = set(train[src].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a row of feautres given two author information\n",
    "colname = [\"npaper_src\", \"npaper_sink\", \"src_first\", \"sink_first\",\"src_last\", \"sink_last\",\n",
    "          \"first_diff\",\"last_diff\",\"overlap_years\",\"common_keywords\",\"keyword_similarity\",\n",
    "           \"common_venue\",\"venue_similarity\",\"common_neighbours\",\"neighbours_similarity\",\"edge\"]\n",
    "colname_str = \"\"\n",
    "for i in range(len(colname)):\n",
    "    colname_str += colname[i]\n",
    "    if i+1 < len(colname):\n",
    "        colname_str += \",\"\n",
    "    else:\n",
    "        colname_str += \"\\n\"\n",
    "\n",
    "def constructRow(src, sink, label=None):\n",
    "    row = []\n",
    "    # num papers\n",
    "    row.append(src['num_papers'])\n",
    "    row.append(sink['num_papers'])\n",
    "    # years since first published\n",
    "    row.append(src['first'])\n",
    "    row.append(sink['first'])\n",
    "    # years since last published\n",
    "    row.append(src['last'])\n",
    "    row.append(sink['last'])\n",
    "    # difference in 'first' between two authors\n",
    "    row.append(src['first'] - sink['first'])\n",
    "    # difference in 'last' between two authors\n",
    "    row.append(src['last'] - sink['last'])\n",
    "    # overlap years\n",
    "    src_range = set(range(src['first'], src['last']))\n",
    "    sink_range = set(range(sink['first'], sink['last']))\n",
    "    row.append(len(src_range.intersection(sink_range)))\n",
    "    \n",
    "    # number of overlapping keywords between two authors\n",
    "    common_keywords = src['keywords'].intersection(sink['keywords'])\n",
    "    row.append(len(common_keywords))\n",
    "    # overlap keywords / union numer of venue\n",
    "    union_keywords = src['keywords'].union(sink['keywords'])\n",
    "    row.append(len(common_keywords) / len(union_keywords))\n",
    "    \n",
    "    # number of overlapping venue between two authors\n",
    "    common_venues = src['venues'].intersection(sink['venues'])\n",
    "    row.append(len(common_venues))\n",
    "    # overlap venue / union numer of venue\n",
    "    union_venues = src['venues'].union(sink['venues'])\n",
    "\n",
    "    if len(union_venues) != 0:\n",
    "        row.append(len(common_venues) / len(union_venues))\n",
    "    else:\n",
    "        row.append(0)\n",
    "    \n",
    "    # number of common neighbours (accounts for nodes that are not recorded in training set)\n",
    "    common_neighbours = train_neighbours.get(src['id'],set([])).intersection(train_neighbours.get(sink['id'],set([])))\n",
    "    row.append(len(common_neighbours))\n",
    "    # overlap neighbours / union number of neighbours\n",
    "    union_neighbours = train_neighbours.get(src['id'],set([])).union(train_neighbours.get(sink['id'],set([])))\n",
    "    \n",
    "    row.append(len(common_neighbours)/len(union_neighbours)) if len(union_neighbours) else row.append(0)\n",
    "\n",
    "    ## dont forget the labelling!\n",
    "    if label:\n",
    "        row.append(label)\n",
    "    else:\n",
    "        row.append(0)\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'npaper_src,npaper_sink,src_first,sink_first,src_last,sink_last,first_diff,last_diff,overlap_years,common_keywords,keyword_similarity,common_venue,venue_similarity,common_neighbours,neighbours_similarity,edge\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colname_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construct table (full)\n",
    "# table = []\n",
    "# # used to avoid repetition of sorce sink pairs\n",
    "# processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# # this nested loop constructs each row of the table\n",
    "# for src_id in author.keys():\n",
    "#     for sink_id in author.keys():\n",
    "#         # dont include edge to self, dont repeat edge\n",
    "#         if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "#             # generate row\n",
    "#             label = train[src_id][sink_id]\n",
    "#             row = constructRow(author[src_id], author[sink_id], label)\n",
    "#             # add row to table\n",
    "#             table.append(row)\n",
    "#             # set edge as processed\n",
    "#             processed[src_id][sink_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct table (stratified sampling, undersampling)\n",
    "table = []\n",
    "# used to avoid repetition of sorce sink pairs\n",
    "processed = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# first add only instance with links\n",
    "for src_id in train.keys():\n",
    "    for sink_id in train[src_id].keys():\n",
    "        # dont include edge to self, dont repeat edge\n",
    "        if (train[src_id][sink_id]) and (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "            # generate row\n",
    "            label = train[src_id][sink_id]\n",
    "            row = constructRow(author[src_id], author[sink_id], label)\n",
    "            # add row to table\n",
    "            table.append(row)\n",
    "            # set edge as processed\n",
    "            processed[src_id][sink_id] = 1\n",
    "\n",
    "# now add instances with no link\n",
    "to_add = len(table)\n",
    "while to_add > 0:\n",
    "    # pick random src and sink\n",
    "    src_id = random.randint(0, len(author.keys())-1)\n",
    "    sink_id = random.randint(0, len(author.keys())-1)\n",
    "    # dont include edge to self, dont repeat edge\n",
    "    if (not src_id == sink_id) and (not processed[src_id][sink_id]):\n",
    "        # generate row\n",
    "        label = train[src_id][sink_id]\n",
    "        row = constructRow(author[src_id], author[sink_id], label)\n",
    "        # add row to table\n",
    "        table.append(row)\n",
    "        # set edge as processed\n",
    "        processed[src_id][sink_id] = 1\n",
    "        \n",
    "    to_add -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('../data/final/train_reconstructed.csv', 'w') as file:\n",
    "    ## header\n",
    "    file.write(colname_str)\n",
    "    for row in table:\n",
    "        row_string = \"\"\n",
    "        for i in range(len(row)):\n",
    "            features = row[i]\n",
    "            row_string += str(features)\n",
    "            if i+1 < len(row):\n",
    "                row_string += \",\"\n",
    "            else:\n",
    "                row_string += \"\\n\"\n",
    "                \n",
    "        file.write(row_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
